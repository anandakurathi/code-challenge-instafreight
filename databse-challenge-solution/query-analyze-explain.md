## Analyze and explain the time complexity of the queries

Include potential suggestions on how to improve it.  
Consider that there might be millions of authors with millions of books.


## Best Practices

Things to keep in mind while writing a good query :

Limit your result –
- When you can not avoid filtering down your select statement, then you can consider limiting your result. By using LIMIT

- Try to keep them simple and efficient. Don’t make the query to be more complex.
    ```Example:
    where id = 77 or id = 85 or id = 69
    instead
    where id in (77, 85, 69)```
-   select * to be avoided. Unnecessary information is waste full of server, database and network resources. When a query joins tables, it drastically increases the result set’s row count, which can lead to a slow execution time.
-   Understand aggregates, stop thinking about loops.  
    Aggregates like –  **count**,  **avg**  etc.
- Indexing is one of the best ways to improve the performance of queries
- Running queries in a loop can significantly slow your runtime.
- The operator classes text_pattern_ops, varchar_pattern_ops, and bpchar_pattern_ops support B-tree indexes on the types text, varchar, and char respectively. The difference from the default operator classes is that the values are compared strictly character by character, rather than according to the locale-specific collation rules. This makes these operator classes suitable for use by queries involving pattern matching expressions LIKE.

## Analyze and explain

SQL statement that executes too slowly, you want to know what is going on and how to fix it. In SQL, it is harder to see how the engine spends its time, because it is not a _procedural language_, but a _declarative language_: you describe the result you want to get, not how to calculate it. The `EXPLAIN` command shows you the _execution plan_ that is generated by the _optimizer_.

### The most important  `EXPLAIN`  options
- `EXPLAIN` will give you the estimated cost, the estimated number of rows and the estimated size of the average result row.
- `ANALYZE`: with this keyword,  `EXPLAIN`  does not only show the plan and PostgreSQL’s estimates, but  **it also executes the query**  (so be careful with  DML) and shows the  _actual_  execution time and row count for each step. This is indispensable for analyzing SQL performance.
  - It tells, The actual execution time in milliseconds, the actual row count and a loop count that shows how often that node was executed. It also shows the number of rows that filters have removed.
- `BUFFERS`: You can only use this keyword together with  `ANALYZE`, and it shows how many 8kB-blocks each step reads, writes and dirties.  **You always want this.**
  - This option shows how many data blocks were found in the cache (`hit`) for each node, how many had to be `read` from disk, how many were `written` and how many `dirtied`.


`EXPLAIN (ANALYZE, BUFFERS)` (with `track_io_timing` turned on) will show you everything you need to know to diagnose SQL statement performance problems. To keep from drowning in a sea of text, you can use [Depesz’](https://explain.depesz.com/) or [Dalibo’s](https://explain.dalibo.com/) visualizer. Both provide roughly the same features.

## Time complexity of the queries
Merge joins generally have a complexity of O(M+N) but it will heavily depend on the indexes on the join columns and, in cases where there is no index, on whether the rows are sorted according to the keys used in the join:

-   If both tables that are sorted according to the keys that are being used in the join, then the query will have a time complexity of O(M+N).
-   If both tables have an index on the joined columns, then the index already maintains those columns in order, and there’s no need to sort. The complexity will

Complexity annotation (such as O ) describes the way an algorithm's performance (e.g. time or space) is dependent on the size of the input. It is "blind" to a specific input. When you write O(m+n) it means that the algorithm will take **O(m) time when m>n and O(n) when m<n**.

If the Query Optimizer chooses to use the index, then it'll first select rows based on the index and then apply a filter with the remaining constraints. Thus reducing the second filtering operation from O(number of rows) to O(number of selected rows by index). The ratio between these two number is called selectivity and an important statistic when choosing which index to use.
